{
  "m": {
    "lcli": {
      "userConfigNotFound": "Fichier de configuration utilisateur non trouvé à {{ .UserConfigPath }}. Un nouveau fichier sera créé.",
      "cfgCreatedSuccessfully": "Fichier de configuration créé avec succès.",
      "deletionConfirm": "Cette action est irréversible. Êtes-vous sûr de vouloir le supprimer ? (y/N) : ",
      "yN": "y",
      "redacted": "[REDACTED]",
      "deletionAborted": "Suppression abandonnée.",
      "a": "a",
      "q": "q"
    },
    "lllm": {
      "processingComplete": "Le traitement de ce fichier est déjà terminé.",
      "anotherInstanceIsProcessing": "Une autre instance traite déjà ce fichier. Veuillez attendre qu'elle se termine ou supprimer le fichier .lock.",
      "ctrlCPressed": "Ctrl+C appuyé. Fin du batch en cours et sauvegarde de la progression. Appuyez à nouveau pour forcer la sortie.",
      "ctrlCPressed2": "La demande de terminaison a déjà été effectuée. Veuillez patienter.",
      "quittingWithoutSaving": "Sortie forcée sans sauvegarder la progression.",
      "processedChunkOf": "Bloc traité {{ .Processed }} sur {{ .Total }}",
      "progressSavedTerminating": "Progression sauvegardée. Arrêt en cours.",
      "progressFileDeleted": "Fichier de progression pour le hachage {{ .Hash }} supprimé.",
      "content": "\n--- Contenu provenant de : {{ .Filename }} ---",
      "filesMerged": "Fichiers correctement fusionnés vers : {{ .MergedFileName }}",
      "openingTag": "Balise d'ouverture",
      "closingTag": "Balise de fermeture",
      "context": "Contexte :",
      "textToTranslate": "Texte à traduire en {{ .LanguageTarget }} :",
      "generatingRequests": "Génération de {{ .Count }} demandes pour le traitement par lots...",
      "sourceEmpty": "Le fichier source est vide ou contient uniquement des espaces blancs. Aucune sortie générée.",
      "wroteEntries": "Écrit avec succès {{ .Count }} entrées dans {{ .TargetPath }}"
    },
    "c": {
      "cfg": {
        "cfgDeletedSuccessfully": "Fichier(s) de configuration supprimé(s) avec succès.",
        "localeSuccessfullyChanged": "La localisation a été changée avec succès en {{ .Locale }}."
      },
      "models": {
        "fallback": "Un preset générique pour toute API compatible OpenAI. Un nom de modèle est requis.",
        "genericprep": "Un preset générique qui préfixe le prompt au texte utilisateur.",
        "genericsys": "Un preset générique qui utilise un prompt système.",
        "noHelp": "Aucun texte d'aide disponible pour ce preset."
      },
      "avg": {
        "averageCharsPerLine": "Moyenne de caractères (graphèmes) par ligne non vide : {{ .AvgChars }}",
        "averageBytesPerLine": "Moyenne de bytes par ligne non vide : {{ .AvgBytes }}"
      },
      "lu": {
        "preparingLaunch": "Préparation du lancement du modèle '{{ .ModelPreset }}' (type : {{ .ModelType }}, quantisation : {{ .QuantReq }}).",
        "found": "Trouvé : {{ .String }}",
        "executingCommand": "\nExécution de la commande : {{ .String }}\n",
        "llamaServerExit": "Le processus llama-server a quitté avec le code {{ .ExitCode }}."
      },
      "rm": {
        "filesToDelete": "Les fichiers de progression suivants seront supprimés :",
        "filesDeletedSuccessfully": "Tous les fichiers de progression ont été supprimés avec succès.",
        "filesToDeletePrompt": "\nEntrez des numéros ou des intervalles à supprimer (ex. `1,3-5`), `a` pour tous, ou `q` pour annuler : ",
        "filesSelectedForDeletion": "\nFichiers sélectionnés pour la suppression :",
        "noValidSelectionsMade": "Aucune sélection valide n'a été faite. Abandon.",
        "noFilesToDelete": "Aucun fichier de progression trouvé à supprimer."
      },
      "sp": {
        "fileSplitSuccess": "Fichier correctement divisé : {{ .SourcePath }}",
        "partCreated": "  - Partie {{ .PartNumber }} créée : {{ .PartPath }}"
      },
      "st": {
        "blockExtracted": "Le contenu entre les délimiteurs a été extrait.",
        "blockDeleted": "Le contenu entre les délimiteurs a été supprimé.",
        "compressed": "Les lignes vides ont été compressées.",
        "newlinesNormalized": "Le contenu du fichier a été normalisé et sauvegardé."
      },
      "tc": {
        "availableModelsForDownload": "Modèles disponibles pour le téléchargement : {{ .AvailableModels }}",
        "downloadingModelFiles": "Téléchargement des fichiers de modèle pour \"{{ .ModelName }}\"...",
        "writingFilesTo": "Écriture des fichiers dans {{ .StateDir }}...",
        "downloadSuccess": "Fichiers téléchargés avec succès pour \"{{ .ModelName }}\":",
        "removedFailedWorker": "Un worker échoué a été supprimé. La taille du pool est maintenant {{ .PoolSize }}."
      },
      "ch": {
        "loadingSession": "Chargement de la session depuis {{ .Path }}...",
        "creatingNewSession": "Création d'une nouvelle session à {{ .Path }}...",
        "defaultSystemPrompt": "Vous êtes un assistant utile.",
        "welcome": "Bienvenue dans la conversation '{{ .ChatName }}' !",
        "typeHelp": "Écrivez un message ou tapez /help pour les commandes.",
        "exiting": "Sortie de la session de chat. Au revoir !",
        "availableCommands": "Commandes disponibles :\n  /exit, /quit    - Quitter la session de chat.\n  /stats          - Afficher les statistiques de la session (comptage des tokens, etc.).\n  /forcegen       - Forcer une nouvelle réponse à partir de la conversation actuelle.\n  /delete <index> - Supprimer le message à l'indice spécifié.\n  /browse         - Entrer dans un navigateur interactif de messages.\n  /insert <path>  - Ajouter un fichier texte ou image à la prochaine message.",
        "deletedMessage": "Message supprimé à l'indice {{ .Index }}.",
        "imageQueued": "Image '{{ .FilePath }}' ajoutée à la prochaine message.",
        "textQueued": "Texte provenant de '{{ .FilePath }}' ajouté à la prochaine message.",
        "imageCountSuffix": " [+{{ .Count }} image(s)]",
        "statsDisplay": "Preset : {{ .Preset }} | Mode : {{ .Mode }} | Modèle : {{ .Model }} | Messages : {{ .Messages }} | Contexte : {{ .Tokens }}/{{ .Limit }} ({{ .Usage }}%)",
        "exitedBrowseMode": "Sortie du mode navigateur.",
        "viewingMessageHeader": "--- VISION DU MESSAGE {{ .Index }} ({{ .Role }}) ---",
        "pressAnyKeyToReturn": "--- Appuyez sur une touche pour revenir à la liste ---",
        "browseModeHeader": "--- MODE NAVIGATEUR ---",
        "browseModeInstructions": "Flèches haut/bas : Navigation | Del : Supprimer | Entrée : Voir | Esc/q : Quitter",
        "statusBarTokens": " Tokens : {{ .Total }}/{{ .Limit }} ({{ .Usage }}%)",
        "statusBarHelp": " Tapez /help pour les commandes ",
        "headerTitle": " Conversation : {{ .SessionName }} ",
        "insertedFileHeader": "\n\n--- Contenu provenant de : {{ .FileName }} ---\n\n"
      }
    }
  },
  "e": {
    "lcli": {
      "unknownErrorOccurred": "Une erreur inconnue s'est produite.",
      "causePrefix": "Cause :",
      "unknownOption": "Option inconnue : '{{ .Option }}'.",
      "unexpectedPositional": "Argument positionnel inattendu : '{{ .Argument }}'.",
      "missingValue": "L'argument de l'option '{{ .Option }}' est manquant.",
      "booleanWithValue": "L'option '{{ .Option }}' ne prend pas d'argument.",
      "ambiguousOptionValue": "La valeur de l'option '{{ .Option }}' est ambigüe. Pour spécifier une valeur commençant par un trait, utilisez '{{ .Option }}=<value>'.",

      "invalidOptionValue": "Valeur invalide fournie pour l'option '{{ .Option }}'.",
      "commandNotImplemented": "Commande non implémentée : {{ .CommandAlias }}.",
      "cfgCouldNotBeLoaded": "Le fichier de configuration ne peut pas être chargé depuis {{ .UserConfigPath }}.",
      "listFormatWidthWarning": "Avertissement : Pas assez d'espace pour formater la description de la liste."
    },
    "v": {
      "invalidArgArray": "Argument array invalide fourni : {{ .OptionValue }}",
      "invalidChunkSize": "Taille du bloc invalide. Doit être un entier entre 1 et 200000. Fourni : {{ .ChunkSize }}",
      "invalidBatchSize": "Taille du lot invalide. Doit être un entier entre 1 et 64. Fourni : {{ .BatchSize }}",
      "invalidIndex": "Indice invalide. Doit être un entier positif. Fourni : {{ .Index }}",
      "invalidPrompt": "Prompt invalide fourni. Doit être une chaîne non vide.",
      "invalidModel": "Nom de modèle invalide fourni : {{ .Model }}",
      "invalidText": "Texte invalide fourni : {{ .Text }}",
      "invalidURL": "URL invalide fournie : {{ .URL }}",
      "invalidURLScheme": "Schéma URL invalide. Doit commencer par http:// ou https://. Fourni : {{ .URL }}",
      "invalidAPIKey": "Clé API invalide fournie : {{ .APIKey }}",
      "invalidTemperatureRange": "La température doit être un nombre entre 0.0 et 2.0.",
      "invalidTopPRange": "Top_p doit être un nombre entre 0.0 et 1.0.",
      "invalidMinPRange": "Min_p doit être un nombre entre 0.0 et 1.0.",
      "invalidTopKRange": "Top_k doit être un entier entre 0 et 1000.",
      "invalidRepeat": "Le coefficient de répétition doit être un nombre entre 1.0 et 2.0.",
      "invalidPenaltyRange": "La pénalité de fréquence/présence doit être un nombre entre -2.0 et 2.0.",
      "seedMustBePositiveInteger": "Le seed doit être un entier positif.",
      "invalidTruthiness": "Valeur de vérité invalide fournie. Doit être true, false, 1 ou 0.",
      "invalidDelayValue": "La valeur de délai doit être un nombre non négatif dans un tuple [boolean, value].",
      "invalidImageArray": "Entrée d'image invalide. Attendu un tableau de URI de données, mais reçu : {{ .Value }}",
      "invalidDataURI": "Format d'image invalide. Attendu une chaîne de données commençant par 'data:', mais reçu : {{ .Value }}",
      "unsupportedImageType": "Avertissement : Le motif d'image {{ .Args }} n'a pas correspondu à aucun fichier supporté.",
      "unsupportedImageType2": "Sauté le type d'image non supporté \"{{ .Ext }}\" pour le fichier : {{ .Image }}",
      "missingImageExtension": "Impossible de déterminer le type d'image pour le fichier '{{ .FilePath }}'. Veuillez assurer que le fichier a une extension valide.",
      "imageNotFound": "Avertissement : Le fichier image non trouvé, sauté : {{ .Image }}"
    },
    "lllm": {
      "undefinedParam": "Paramètre non défini.",
      "reasoningNotSupported": "Le modèle ne supporte pas la réflexion.",
      "invalidReasoningType": "Type de réflexion invalide, configuration JSON corrompue.",
      "promptMissing": "Le prompt est requis pour la commande oneshot.",
      "fileNotFound": "Fichier non trouvé : {{ .FilePath }}",
      "sourceRequired": "Le chemin source est requis.",
      "sourceTargetRequired": "Les chemins source et cible sont requis.",
      "noFilesFound": "Aucun fichier trouvé avec l'extension : .{{ .Extension }}",
      "targetFileExists": "Le chemin cible existe déjà : {{ .TargetPath }}",
      "sourceAndTargetMustBeDifferent": "Les chemins source et cible doivent être différents.",
      "invalidFileSize": "La taille du fichier dépasse la limite maximale de {{ .MAX_SIZE_MB }} MB.",
      "emptyFile": "Le fichier est vide ou contient uniquement des espaces blancs.",
      "idleTimeOut": "Dépassement du délai d'inactivité. Aucune donnée n'a été reçue du serveur.",
      "hardTimeOut": "Dépassement du délai dur. La requête a pris trop de temps pour être complétée.",
      "tExceeded": "Dépassement du délai d'inactivité",
      "unknownOpenAIError": "Une erreur inconnue s'est produite avec l'API.",
      "openaiApiError": "L'appel à l'API a échoué avec le code {{ .Status }} : {{ .Message }}",
      "networkErrorOpenAI": "Erreur réseau lors de l'appel à {{ .URL }}.",
      "networkErrorReason": " Raison : {{ .Code }}",
      "responseNull": "Le corps de réponse est null",
      "streamEndedPrematurely": "Le flux s'est terminé prématurément sans signal [DONE].",
      "badPromptConfig": "Configuration du prompt invalide. Un prompt est requis.",
      "progressFileDoesNotExist": "Le fichier de progression pour le hachage {{ .Hash }} n'existe pas.",
      "llmAPICall": "Erreur lors de l'appel à l'API LLM : ",
      "initializingBatch": "Erreur lors de l'initialisation du traitement par lots.",
      "failedLock": "Échec de la création du fichier de verrouillage.",
      "failedToSaveProgress": "Échec de la sauvegarde de la progression.",
      "whileCalling_deleteProgressEntry": " pendant l'appel à deleteProgressEntry.",
      "stripNewLinesTypeError": "L'entrée doit être une chaîne ou un tableau de chaînes.",
      "invalidFormat": "Format invalide '{{ .Format }}'. Les formats pris en charge sont 'openai' et 'gemini'.",
      "jsonlGenError": "Une erreur s'est produite lors de la génération du fichier JSONL.",
      "idleTimeoutExceeded": "Dépassement du délai d'inactivité"
    },
    "c": {
      "co": {
        "coError": "Avertissement : Impossible de charger le module pour la commande {{ .Command }}. Ignorée la génération de complétion."
      },
      "cfg": {
        "editorNotFound": "Variable d'environnement $EDITOR non trouvée. Veuillez définir $EDITOR sur votre éditeur préféré.",
        "editorLaunchFailed": "Échec du lancement de l'éditeur : {{ .ErrorMessage }}",
        "failedToWriteLocale": "Échec de l'écriture du fichier de localisation : {{ .ErrorMessage }}",
        "invalidLocale": "Localisation invalide : {{ .Lang }}."
      },
      "lu": {
        "undefinedLauncher": "Le modèle '{{ .ModelPreset }}' n'est pas configuré pour un lancement local (manque 'quantFiles' ou 'quantizationOrder').",
        "undefinedModelType": "Le type de modèle '{{ .ModelType }}' n'existe pas pour le preset '{{ .ModelPreset }}'.",
        "undefinedQuant": "La quantisation '{{ .QuantReq }}' n'est pas définie pour le preset du modèle '{{ .ModelPreset }}'. Disponible : {{ .AvailableQuants }}.",
        "noModelFiles": "Aucun fichier de modèle GGUF trouvé sur disque pour le preset '{{ .ModelPreset }}' (type : {{ .ModelType }}). Vérifié les quantisations : {{ .QuantList }}.",
        "failedToStart": "Échec du lancement du processus llama-server : {{ .Error }}."
      },
      "mg": {
        "extensionRequired": "L'extension du fichier est requise. Utilisez le flag -e ou --extension."
      },
      "rm": {
        "unknownMode": "Mode inconnu"
      },
      "sp": {
        "invalidSplitSize": "Taille invalide : {{ .Size }}. Doit être un nombre positif."
      },
      "st": {
        "delimiterPairRequired": "Les délimiteurs d'ouverture et de fermeture sont requis."
      },
      "tc": {
        "tokenizerDoesNotExist": "Le tokenizer pour le preset '{{ .PresetName }}' n'existe pas.",
        "modelNotFoundForDownload": "Le modèle \"{{ .ModelName }}\" n'a pas été trouvé.",
        "failedToDownload": "Échec du téléchargement de {{ .ModelUrl }} : {{ .Status }} {{ .StatusText }}",
        "modelDownloadError": "Erreur lors du téléchargement du modèle : {{ .ErrorMessage }}",
        "tokenizerLoadFailed": "Échec du chargement des données du tokenizer pour \"{{ .TokenizerName }}\". Vérifiez que '{{ .JsonPath }}' et '{{ .ConfigPath }}' existent et sont des fichiers JSON valides. Erreur : {{ .Error }}",
        "tokenizerFilesNotFound": "Échec du chargement des fichiers tokenizer pour {{ .TokenizerName }}",
        "unhandledWorkerError": "Erreur non gérée dans le worker tokenizer : {{ .Message }}",
        "poolShuttingDown": "Le pool de workers est en cours de fermeture.",
        "poolShutdownJobCancelled": "Le pool de workers est en cours de fermeture. La tâche {{ .JobID }} a été annulée."
      },
      "ch": {
        "chatNameMissing": "Erreur : Un nom pour la session de chat est requis.",
        "sessionLoadError": "Erreur : Impossible de charger ou d'analyser le fichier de session à {{ .Path }}.",
        "chatLoopError": "Une erreur inattendue s'est produite pendant la session de chat : {{ .Error }}",
        "contextLimitExceeded": "Limite de contexte de {{ .Limit }} tokens atteinte (actuellement à {{ .Total }}). Utilisez /delete <index> ou /browse pour supprimer des messages.",
        "invalidDeleteIndex": "Erreur : Indice de message invalide. Veuillez fournir un nombre entre 1 et {{ .Count }}.",
        "unknownCommand": "Erreur : Commande inconnue '/{{ .Command }}'. Tapez /help pour obtenir la liste des commandes.",
        "insertUsage": "Utilisation : /insert <chemin_fichier>"
      }
    }
  },
  "help": {
    "generic": {
      "header": "telocity : Un outil pour le traitement en lots de texte avec des modèles LLM.",
      "usage": "Utilisation : telocity <commande> [options]",
      "commandHeader": "Commandes :",
      "commandDescriptions": {
        "lu": "Démarrer le serveur llama avec un preset configuré.",
        "tr": "Traduire un fichier par blocs.",
        "tf": "Appliquer un prompt de transformation à un fichier par blocs.",
        "os": "Exécuter un prompt unique avec contexte facultatif de fichier.",
        "ch": "Démarrer une session interactive avec un modèle LLM.",
        "bg": "Générer un fichier JSONL pour le traitement en lots.",
        "st": "Enlever ou extraire du contenu entre des délimiteurs dans un fichier.",
        "mg": "Fusionner plusieurs fichiers texte en un seul fichier.",
        "sp": "Diviser un grand fichier en parties plus petites.",
        "avg": "Calculer la longueur moyenne d'une ligne dans un fichier.",
        "tc": "Compter les tokens dans un fichier pour un modèle spécifique.",
        "rm": "Supprimer les fichiers de progression pour des tâches terminées ou bloquées.",
        "cfg": "Gérer la configuration de l'application."
      },
      "footer": "Pour plus d'informations sur une commande, utilisez `telocity <commande> --help`.",
      "globalOptionsHeader": "Options globales :",
      "flags": {
        "version": "Afficher la version de l'application."
      }
    },
    "commands": {
      "lu": {
        "usage": "Utilisation : telocity launch [model_preset] [options]",
        "description": "Recherche le fichier de modèle local disponible le plus adapté pour un preset donné et lance le serveur llama.cpp avec les paramètres appropriés.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "reason": "Charger la version 'thinking' ou 'reasoning' du modèle, si disponible.",
          "chat": "Lancer en mode chat pour un seul utilisateur (omite les arguments de traitement parallèle).",
          "quant": "Forcer l'utilisation d'un niveau spécifique de quantification (ex. 'q8', 'q6')."
        },
        "footer": "Modèles disponibles à lancer :\n{{ .LaunchableModelList }}"
      },
      "avg": {
        "usage": "Utilisation : telocity avg <source_path>",
        "description": "Calcule le nombre moyen de caractères (graphèmes) et d'octets par ligne non vide dans un fichier."
      },
      "cfg": {
        "usage": "Utilisation : telocity cfg [options]",
        "description": "Gère la configuration de l'application.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "edit": "Ouvrir le fichier de configuration utilisateur dans l'éditeur par défaut.",
          "remove": "Supprimer les fichiers de configuration utilisateur (confirmation requise).",
          "lang": "Définir la langue de l'application."
        },
        "footer": "Localisations prises en charge :\n{{ .LocaleList }}"
      },
      "rm": {
        "usage": "Utilisation : telocity rm <source_path> [options]",
        "description": "Supprime les fichiers de suivi de progression. La commande supporte trois modes :\n\n- Positionnel (par défaut) : Supprime l'entrée de progression pour un seul fichier source, identifié par <source_path>.\n- Tous (--all) : Supprime tous les fichiers de progression actuellement suivis.\n- Interactif (--interactive) : Ouvre un menu qui liste tous les fichiers de progression, permettant de sélectionner spécifiquement ceux à supprimer.\n\nUtilisez --force (-f) pour ignorer les prompts de confirmation.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "all": "Supprimer tous les fichiers de progression.",
          "force": "Suppression forcée sans confirmation.",
          "interactive": "Ouvrir un menu interactif pour sélectionner les fichiers de progression à supprimer."
        }
      },
      "mg": {
        "usage": "Utilisation : telocity mg <source_directory> [target_directory] -e <extension>",
        "description": "Recherche récursivement et fusionne tous les fichiers avec une extension donnée depuis un dossier source vers un seul fichier dans un dossier cible (ou dans le dossier actuel si non spécifié).",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "extension": "L'extension du fichier à rechercher (obligatoire)."
        }
      },
      "os": {
        "usage": "Utilisation : telocity os \"<prompt>\" [options]",
        "description": "Exécute un prompt unique contre un modèle LLM. Il peut prendre du contexte depuis un fichier (--file), l'entrée standard (stdin) et des images (--image), puis transmet le flux de sortie du modèle vers le terminal.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "file": "Chemin vers un fichier texte à ajouter au prompt comme contexte.",
          "image": "Chemin vers une image ou plusieurs images pour le contexte visuel. Supporte les motifs globaux et des listes séparées par des virgules (ex. \"img1.jpg,*.png\"). Formats pris en charge : png, jpg/jpeg, gif, webp.",
          "params": "Sélectionner un preset de paramètres du modèle (par défaut : {{ .DefaultModel }}).",
          "model": "Surcharger le nom du modèle spécifié dans le preset.",
          "url": "Surcharger l'URL de l'API.",
          "apikey": "Fournir une clé API pour la requête.",
          "reason": "Activer le mode de réflexion pour les presets qui le supportent."
        },
        "footer": "Prestations disponibles :\n{{ .ModelParamList }}"
      },
      "sp": {
        "usage": "Utilisation : telocity sp <source_path> <target_directory> [options]",
        "description": "Divise un grand fichier en parties plus petites selon une taille spécifiée, en veillant à ce que les lignes ne soient pas coupées.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "size": "La taille maximale de chaque partie en mégaoctets (par défaut : {{ .Size }})."
        }
      },
      "st": {
        "usage": "Utilisation : telocity st <source_path> <target_path> [options]",
        "description": "Enlève ou extrait du contenu entre des délimiteurs spécifiés dans un fichier.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "startdelimiter": "Le délimiteur d'ouverture.",
          "enddelimiter": "Le délimiteur de fermeture.",
          "params": "Utiliser les délimiteurs d'ouverture/fermeture depuis un preset.",
          "extracttag": "Extraire le contenu entre les délimiteurs au lieu de le supprimer.",
          "compress": "Compresser les lignes vides dans la sortie."
        },
        "footer": "Prestations disponibles avec des balises :\n{{ .ReasoningTagParamList }}"
      },
      "tc": {
        "usage": "Utilisation : telocity tc <source_path> [options]",
        "description": "Compte le nombre de tokens dans un fichier en utilisant un tokenizer spécifique au modèle.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "params": "Le preset du modèle dont le tokenizer doit être utilisé (par défaut : {{ .DefaultModel }}).",
          "downloadmodel": "Télécharger les fichiers de tokenizer pour un modèle spécifique (ex. 'qwen')."
        },
        "footer": "Tokenizers disponibles :\n{{ .TokenParamList }}"
      },
      "tf": {
        "usage": "Utilisation : telocity tf <source_path> <target_path> [options]",
        "description": "Applique un prompt de transformation à un fichier source, le traitant par blocs. Lors de l'utilisation de `--image`, le fichier source doit contenir le prompt pour les images. Dans ce mode, évitez de définir une taille personnalisée `--chunksize` afin que le prompt soit traité comme unité unique.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "chunksize": "Nombre de lignes par bloc (par défaut : {{ .ChunkSize }}).",
          "batchsize": "Nombre de blocs à traiter par lot (par défaut : {{ .BatchSize }}).",
          "parallel": "Limite de concurrence (par défaut : {{ .Parallel }}).",
          "prompt": "Un instruction optionnelle à préfixer au contenu du fichier source. Généralement omis lors de l'utilisation du flag --image.",
          "image": "Chemin vers une image ou plusieurs images pour le contexte visuel. Supporte les motifs globaux et des listes séparées par des virgules (ex. \"img1.jpg,*.png\"). Formats pris en charge : png, jpg/jpeg, gif, webp.",
          "sysprompt": "Définir un prompt système personnalisé.",
          "params": "Sélectionner un preset de paramètres du modèle (par défaut : {{ .DefaultModel }}).",
          "model": "Surcharger le nom du modèle provenant du preset.",
          "url": "Surcharger l'URL de l'API.",
          "apikey": "Fournir une clé API.",
          "wait": "Définir un délai entre les appels à l'API.",
          "reason": "Activer le mode de réflexion pour les presets qui le supportent."
        },
        "footer": "Prestations disponibles :\n{{ .ModelParamList }}"
      },
      "tr": {
        "usage": "Utilisation : telocity tr <source_path> <target_path> [options]",
        "description": "Traduit un fichier d'une langue source vers une langue cible, le traitant par blocs.",
        "flags": {
          "help": "Afficher ce message d'aide.",
          "chunksize": "Nombre de lignes par bloc (par défaut : {{ .ChunkSize }}).",
          "batchsize": "Nombre de blocs à traiter par lot (par défaut : {{ .BatchSize }}).",
          "parallel": "Limite de concurrence (par défaut : {{ .Parallel }}).",
          "source": "Langue source (par défaut : \"{{ .SourceLanguage }}\").",
          "target": "Langue cible (par défaut : \"{{ .TargetLanguage }}\").",
          "context": "Fournir un contexte supplémentaire pour la traduction.",
          "params": "Sélectionner un preset de paramètres du modèle (par défaut : {{ .DefaultModel }}).",
          "model": "Surcharger le nom du modèle provenant du preset.",
          "url": "Surcharger l'URL de l'API.",
          "apikey": "Fournir une clé API.",
          "wait": "Définir un délai entre les appels à l'API.",
          "reason": "Activer le mode de réflexion pour les presets qui le supportent."
        },
        "footer": "Prestations disponibles :\n{{ .ModelParamList }}"
      },
      "bg": {
        "usage": "Utilisation : telocity bg [options] <source_file> <target_jsonl_file>",
        "description": "Génère un fichier JSONL pour le traitement en lots de traduction à partir d'un fichier texte source.",
        "flags": {
          "format": "Le format d'output du fichier JSONL. Supporté : 'openai', 'gemini'. (par défaut : openai)",
          "chunksize": "Nombre de lignes par bloc/requete. (par défaut : {{ .ChunkSize }})",
          "model": "Remplace le modèle spécifié dans le preset.",
          "params": "Preset à utiliser pour la configuration du prompt et du modèle. (par défaut : {{ .DefaultModel }})",
          "source": "Langue source pour les prompts de traduction.",
          "target": "Langue cible pour les prompts de traduction.",
          "context": "Contexte supplémentaire à inclure dans le prompt pour chaque bloc.",
          "reason": "Utiliser la variante 'reasoning' du preset de modèle sélectionné.",
          "help": "Afficher ce message d'aide."
        },
        "footer": "\nPrestations disponibles :\n{{ .ModelParamList }}"
      },
      "ch": {
        "usage": "Utilisation : llm-cli ch [options] <chat-name>",
        "description": "Démarrer une session interactive avec un modèle LLM. L'historique de chat et la configuration sont sauvegardés dans un fichier nommé <chat-name>.json dans le répertoire d'état de l'application.",
        "flags": {
          "file": "Ajoute un fichier texte au premier message d'une nouvelle session de chat.",
          "image": "Ajoute une ou plusieurs images (séparées par des virgules ou motif globale) au premier message d'une nouvelle session de chat.",
          "model": "Remplace le modèle spécifié par le preset.",
          "params": "Le preset de paramètres à utiliser pour le modèle LLM. Voir la liste ci-dessous.",
          "context-limit": "Définir la limite maximale de tokens pour le contexte du chat.",
          "apikey": "Clé API pour le service LLM.",
          "url": "Remplacer l'URL pour le service LLM.",
          "help": "Afficher ce message d'aide."
        },
        "footer": "Dans le chat, tapez /help pour voir les commandes interactives disponibles comme /exit, /stats, /delete, /browse et /insert.\n\nPrestations disponibles :\n{{ .ModelParamList }}"
      }
    }
  }
}
