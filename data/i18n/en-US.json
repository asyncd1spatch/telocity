{
  "m": {
    "lcli": {
      "userConfigNotFound": "User configuration file not found at {{ .UserConfigPath }}. A new one will be created.",
      "cfgCreatedSuccessfully": "Configuration file created successfully.",
      "deletionConfirm": "This action is irreversible. Are you sure you want to delete? (y/N): ",
      "yN": "y",
      "redacted": "[REDACTED]",
      "deletionAborted": "Deletion aborted.",
      "a": "a",
      "q": "q"
    },
    "lllm": {
      "processingComplete": "Processing for this file is already complete.",
      "anotherInstanceIsProcessing": "Another instance is already processing this file. Please wait for it to finish or remove the .lock file.",
      "ctrlCPressed": "Ctrl+C pressed. Finishing current batch and saving progress. Press again to force quit.",
      "ctrlCPressed2": "Termination already requested. Please wait.",
      "quittingWithoutSaving": "Force quitting without saving progress.",
      "processedChunkOf": "Processed chunk {{ .Processed }} of {{ .Total }}",
      "progressSavedTerminating": "Progress saved. Terminating.",
      "progressFileDeleted": "Progress file for hash {{ .Hash }} deleted.",
      "content": "\n--- Content from: {{ .Filename }} ---",
      "filesMerged": "Files successfully merged into: {{ .MergedFileName }}",
      "openingTag": "Opening tag",
      "closingTag": "Closing tag",
      "context": "Context:",
      "textToTranslate": "Text to translate to {{ .LanguageTarget }}:",
      "generatingRequests": "Generating {{ .Count }} requests for batch processing...",
      "sourceEmpty": "Source file is empty or contains only whitespace. No output generated.",
      "wroteEntries": "Successfully wrote {{ .Count }} entries to {{ .TargetPath }}"
    },
    "c": {
      "cfg": {
        "cfgDeletedSuccessfully": "Configuration file(s) deleted successfully.",
        "localeSuccessfullyChanged": "Locale successfully changed to {{ .Locale }}."
      },
      "models": {
        "fallback": "A generic preset for any OpenAI-compatible API. Requires a model name.",
        "genericprep": "A generic preset that prepends the prompt to the user's text.",
        "genericsys": "A generic preset that uses a system prompt.",
        "noHelp": "No help text available for this preset."
      },
      "avg": {
        "averageCharsPerLine": "Average characters (graphemes) per non-empty line: {{ .AvgChars }}",
        "averageBytesPerLine": "Average bytes per non-empty line: {{ .AvgBytes }}"
      },
      "lu": {
        "preparingLaunch": "Preparing to launch model '{{ .ModelPreset }}' (type: {{ .ModelType }}, quant: {{ .QuantReq }}).",
        "found": "Found: {{ .String }}",
        "executingCommand": "\nExecuting Command: {{ .String }}\n",
        "llamaServerExit": "llama-server process exited with code {{ .ExitCode }}."
      },
      "rm": {
        "filesToDelete": "The following progress files will be deleted:",
        "filesDeletedSuccessfully": "All progress files deleted successfully.",
        "filesToDeletePrompt": "\nEnter numbers or ranges to delete (e.g. `1,3-5`), `a` for all, or `q` to cancel: ",
        "filesSelectedForDeletion": "\nFiles selected for deletion:",
        "noValidSelectionsMade": "No valid selections made. Aborting.",
        "noFilesToDelete": "No progress files found to delete."
      },
      "sp": {
        "fileSplitSuccess": "Successfully split file: {{ .SourcePath }}",
        "partCreated": "  - Part {{ .PartNumber }} created: {{ .PartPath }}"
      },
      "st": {
        "blockExtracted": "Content between delimiters extracted.",
        "blockDeleted": "Content between delimiters deleted.",
        "compressed": "Empty lines compressed.",
        "newlinesNormalized": "File content normalized and saved."
      },
      "tc": {
        "availableModelsForDownload": "Available models for download: {{ .AvailableModels }}",
        "downloadingModelFiles": "Downloading model files for \"{{ .ModelName }}\"...",
        "writingFilesTo": "Writing files to {{ .StateDir }}...",
        "downloadSuccess": "Successfully downloaded files for \"{{ .ModelName }}\":",
        "removedFailedWorker": "Removed a failed worker. Pool size is now {{ .PoolSize }}."
      },
      "ch": {
        "loadingSession": "Loading session from {{ .Path }}...",
        "creatingNewSession": "Creating new session at {{ .Path }}...",
        "defaultSystemPrompt": "You are a helpful assistant.",
        "welcome": "Welcome to chat '{{ .ChatName }}'!",
        "typeHelp": "Type a message, or type /help for commands.",
        "exiting": "Exiting chat session. Goodbye!",
        "availableCommands": "Available commands:\n  /exit, /quit    - Exit the chat session.\n  /stats          - Show session statistics (token count, etc.).\n  /forcegen       - Force a new response from the current conversation.\n  /delete <index> - Delete the message at the specified index.\n  /browse         - Enter interactive message browser.\n  /insert <path>  - Queue a text or image file to be included in the next message.",
        "deletedMessage": "Deleted message at index {{ .Index }}.",
        "imageQueued": "Image '{{ .FilePath }}' queued for next message.",
        "textQueued": "Text from '{{ .FilePath }}' queued for next message.",
        "imageCountSuffix": " [+{{ .Count }} image(s)]",
        "statsDisplay": "Preset: {{ .Preset }} | Mode: {{ .Mode }} | Model: {{ .Model }} | Messages: {{ .Messages }} | Context: {{ .Tokens }}/{{ .Limit }} ({{ .Usage }}%)",
        "exitedBrowseMode": "Exited browse mode.",
        "viewingMessageHeader": "--- VIEWING MESSAGE {{ .Index }} ({{ .Role }}) ---",
        "pressAnyKeyToReturn": "--- Press any key to return to list ---",
        "browseModeHeader": "--- BROWSE MODE ---",
        "browseModeInstructions": "Up/Down: Navigate | Del: Delete | Enter: View | Esc/q: Exit",
        "statusBarTokens": " Tokens: {{ .Total }}/{{ .Limit }} ({{ .Usage }}%)",
        "statusBarHelp": " Type /help for commands ",
        "headerTitle": " Chatting: {{ .SessionName }} ",
        "insertedFileHeader": "\n\n--- Content from: {{ .FileName }} ---\n\n"
      }
    }
  },
  "e": {
    "lcli": {
      "unknownErrorOccurred": "An unknown error occurred.",
      "causePrefix": "Cause:",
      "unknownOption": "Unknown option: '{{ .Option }}'.",
      "unexpectedPositional": "Unexpected positional argument: '{{ .Argument }}'.",
      "missingValue": "Option '{{ .Option }}' argument is missing.",
      "booleanWithValue": "Option '{{ .Option }}' does not take an argument.",
      "ambiguousOptionValue": "Option '{{ .Option }}' argument is ambiguous. To specify a value that starts with a dash, use '{{ .Option }}=<value>'.",

      "invalidOptionValue": "Invalid value provided for option '{{ .Option }}'.",
      "commandNotImplemented": "Command not implemented: {{ .CommandAlias }}.",
      "cfgCouldNotBeLoaded": "Configuration file could not be loaded from {{ .UserConfigPath }}.",
      "listFormatWidthWarning": "Warning: Not enough space to format list description."
    },
    "v": {
      "invalidArgArray": "Invalid argument array provided: {{ .OptionValue }}",
      "invalidChunkSize": "Invalid chunk size. Must be an integer between 1 and 200000. Provided: {{ .ChunkSize }}",
      "invalidBatchSize": "Invalid batch size. Must be an integer between 1 and 64. Provided: {{ .BatchSize }}",
      "invalidIndex": "Invalid lastIndex. Must be a positive integer. Provided: {{ .Index }}",
      "invalidPrompt": "Invalid prompt provided. Must be a non-empty string.",
      "invalidModel": "Invalid model name provided: {{ .Model }}",
      "invalidText": "Invalid text provided: {{ .Text }}",
      "invalidURL": "Invalid URL provided: {{ .URL }}",
      "invalidURLScheme": "Invalid URL scheme. Must start with http:// or https://. Provided: {{ .URL }}",
      "invalidAPIKey": "Invalid API key provided: {{ .APIKey }}",
      "invalidTemperatureRange": "Temperature must be a number between 0.0 and 2.0.",
      "invalidTopPRange": "Top_p must be a number between 0.0 and 1.0.",
      "invalidMinPRange": "Min_p must be a number between 0.0 and 1.0.",
      "invalidTopKRange": "Top_k must be an integer between 0 and 1000.",
      "invalidRepeat": "Repeat penalty must be a number between 1.0 and 2.0.",
      "invalidPenaltyRange": "Frequency/Presence penalty must be a number between -2.0 and 2.0.",
      "seedMustBePositiveInteger": "Seed must be a positive integer.",
      "invalidTruthiness": "Invalid truthiness value provided. Must be true, false, 1, or 0.",
      "invalidDelayValue": "Delay must be a non-negative number within a [boolean, value] tuple.",
      "invalidImageArray": "Invalid image input. Expected an array of data URIs, but received: {{ .Value }}",
      "invalidDataURI": "Invalid image format. Expected a data URI string starting with 'data:', but received: {{ .Value }}",
      "unsupportedImageType": "Warning: The image glob pattern {{ .Args }} did not match any supported files.",
      "unsupportedImageType2": "Skipping unsupported image type \"{{ .Ext }}\" for file: {{ .Image }}",
      "missingImageExtension": "Could not determine image type for file '{{ .FilePath }}'. Please ensure the file has a valid extension.",
      "imageNotFound": "Warning: Image file not found, skipping: {{ .Image }}"
    },
    "lllm": {
      "undefinedParam": "Undefined parameter set.",
      "reasoningNotSupported": "Model does not support reasoning.",
      "invalidReasoningType": "Invalid reasoning type, broken JSON config.",
      "promptMissing": "Prompt is required for the oneshot command.",
      "fileNotFound": "File not found: {{ .FilePath }}",
      "sourceRequired": "Source path is required.",
      "sourceTargetRequired": "Source and target paths are required.",
      "noFilesFound": "No files found with extension: .{{ .Extension }}",
      "targetFileExists": "Target path already exists: {{ .TargetPath }}",
      "sourceAndTargetMustBeDifferent": "Source and target paths must be different.",
      "invalidFileSize": "File size exceeds the maximum limit of {{ .MAX_SIZE_MB }} MB.",
      "emptyFile": "File is empty or contains only whitespace.",
      "idleTimeOut": "Idle timeout exceeded. No data received from the server.",
      "hardTimeOut": "Hard timeout exceeded. The request took too long to complete.",
      "tExceeded": "Idle timeout exceeded",
      "unknownOpenAIError": "An unknown error occurred with the API.",
      "openaiApiError": "API call failed with status {{ .Status }}: {{ .Message }}",
      "networkErrorOpenAI": "Network error when calling {{ .URL }}.",
      "networkErrorReason": " Reason: {{ .Code }}",
      "responseNull": "Response body is null",
      "streamEndedPrematurely": "Stream ended prematurely without a [DONE] signal.",
      "badPromptConfig": "Bad prompt configuration. A prompt is required.",
      "progressFileDoesNotExist": "Progress file for hash {{ .Hash }} does not exist.",
      "llmAPICall": "Error during LLM API calls: ",
      "initializingBatch": "Error initializing batch processing.",
      "failedLock": "Failed to create lock file.",
      "failedToSaveProgress": "Failed to save progress.",
      "whileCalling_deleteProgressEntry": " while calling deleteProgressEntry.",
      "stripNewLinesTypeError": "Input must be a string or an array of strings.",
      "invalidFormat": "Invalid format '{{ .Format }}'. Supported formats are 'openai' and 'gemini'.",
      "jsonlGenError": "An error occurred during JSONL file generation.",
      "idleTimeoutExceeded": "Idle timeout exceeded"
    },
    "c": {
      "co": {
        "coError": "Warning: Could not load module for command {{ .Command }}. Skipping completion generation."
      },
      "cfg": {
        "editorNotFound": "No $EDITOR environment variable found. Please set $EDITOR to your preferred text editor.",
        "editorLaunchFailed": "Failed to launch editor: {{ .ErrorMessage }}",
        "failedToWriteLocale": "Failed to write locale file: {{ .ErrorMessage }}",
        "invalidLocale": "Invalid locale: {{ .Lang }}."
      },
      "lu": {
        "undefinedLauncher": "Model '{{ .ModelPreset }}' is not configured for local launch (missing 'quantFiles' or 'quantizationOrder').",
        "undefinedModelType": "Model type '{{ .ModelType }}' not found for preset '{{ .ModelPreset }}'.",
        "undefinedQuant": "Quantization '{{ .QuantReq }}' is not defined for model preset '{{ .ModelPreset }}'. Available: {{ .AvailableQuants }}.",
        "noModelFiles": "No GGUF model files found on disk for preset '{{ .ModelPreset }}' (type: {{ .ModelType }}). Checked quants: {{ .QuantList }}.",
        "failedToStart": "Failed to start llama-server process: {{ .Error }}."
      },
      "mg": {
        "extensionRequired": "File extension is required. Use the -e or --extension flag."
      },
      "rm": {
        "unknownMode": "Unknown mode"
      },
      "sp": {
        "invalidSplitSize": "Invalid size: {{ .Size }}. Must be a positive number."
      },
      "st": {
        "delimiterPairRequired": "Both start and end delimiters are required."
      },
      "tc": {
        "tokenizerDoesNotExist": "Tokenizer for preset '{{ .PresetName }}' does not exist.",
        "modelNotFoundForDownload": "Model \"{{ .ModelName }}\" not found.",
        "failedToDownload": "Failed to download {{ .ModelUrl }}: {{ .Status }} {{ .StatusText }}",
        "modelDownloadError": "Error during model download: {{ .ErrorMessage }}",
        "tokenizerLoadFailed": "Failed to load tokenizer data for \"{{ .TokenizerName }}\". Ensure '{{ .JsonPath }}' and '{{ .ConfigPath }}' exist and are valid JSON files. Error: {{ .Error }}",
        "tokenizerFilesNotFound": "Failed to load tokenizer files for {{ .TokenizerName }}",
        "unhandledWorkerError": "Unhandled error in token worker: {{ .Message }}",
        "poolShuttingDown": "Worker pool is shutting down.",
        "poolShutdownJobCancelled": "Worker pool is shutting down. Job {{ .JobID }} cancelled."
      },
      "ch": {
        "chatNameMissing": "Error: A name for the chat session is required.",
        "sessionLoadError": "Error: Could not load or parse the session file at {{ .Path }}.",
        "chatLoopError": "An unexpected error occurred during the chat session: {{ .Error }}",
        "contextLimitExceeded": "Context limit of {{ .Limit }} tokens reached (currently at {{ .Total }}). Use /delete <index> or /browse to remove messages.",
        "invalidDeleteIndex": "Error: Invalid message index. Please provide a number between 1 and {{ .Count }}.",
        "unknownCommand": "Error: Unknown command '/{{ .Command }}'. Type /help for a list of commands.",
        "insertUsage": "Usage: /insert <file-path>"
      }
    }
  },
  "help": {
    "generic": {
      "header": "telocity: A tool for batch processing text with LLMs.",
      "usage": "Usage: telocity <command> [options]",
      "commandHeader": "Commands:",
      "commandDescriptions": {
        "lu": "Launch llama server using a configured preset.",
        "tr": "Translate a file chunk by chunk.",
        "tf": "Apply a transformation prompt to a file chunk by chunk.",
        "os": "Execute a single prompt with optional file context.",
        "ch": "Starts an interactive chat session with an LLM.",
        "bg": "Generate a JSONL file for batch processing.",
        "st": "Strip or extract content between delimiters in a file.",
        "mg": "Merge multiple text files into a single file.",
        "sp": "Split a large file into smaller parts.",
        "avg": "Calculate average line length of a file.",
        "tc": "Count tokens in a file for a specific model.",
        "rm": "Remove progress files for completed or stuck jobs.",
        "cfg": "Manage application configuration."
      },
      "footer": "For more information on any command, use `telocity <command> --help`.",
      "globalOptionsHeader": "Global Options:",
      "flags": {
        "version": "Show application version."
      }
    },
    "commands": {
      "lu": {
        "usage": "Usage: telocity launch [model_preset] [options]",
        "description": "Finds the best available local model file for a given preset and launches the llama.cpp server with the appropriate parameters.",
        "flags": {
          "help": "Show this help message.",
          "reason": "Load the 'thinking' or 'reasoning' version of the model, if available.",
          "chat": "Launch in chat mode for a single user (omits parallel processing arguments).",
          "quant": "Force the use of a specific quantization level (e.g., 'q8', 'q6')."
        },
        "footer": "Available Launchable Models:\n{{ .LaunchableModelList }}"
      },
      "avg": {
        "usage": "Usage: telocity avg <source_path>",
        "description": "Calculates the average number of characters (graphemes) and bytes per non-empty line in a file."
      },
      "cfg": {
        "usage": "Usage: telocity cfg [options]",
        "description": "Manages the application configuration.",
        "flags": {
          "help": "Show this help message.",
          "edit": "Open the user configuration file in the default editor.",
          "remove": "Delete the user configuration files (prompts for confirmation).",
          "lang": "Set the application language."
        },
        "footer": "Supported locales:\n{{ .LocaleList }}"
      },
      "rm": {
        "usage": "Usage: telocity rm <source_path> [options]",
        "description": "Removes progress-tracking files. The command supports three modes:\n\n- Positional (default): Removes the progress entry for a single source file, identified by <source_path>.\n- All (--all): Removes all progress files currently tracked.\n- Interactive (--interactive): Opens a menu that lists all progress files, allowing you to select specific ones to delete.\n\nUse --force (-f) to skip confirmation prompts.",
        "flags": {
          "help": "Show this help message.",
          "all": "Remove all progress files.",
          "force": "Force deletion without confirmation.",
          "interactive": "Open an interactive menu to select which progress files to delete."
        }
      },
      "mg": {
        "usage": "Usage: telocity mg <source_directory> [target_directory] -e <extension>",
        "description": "Recursively finds and merges all files with a given extension from a source directory into a single file in the target directory (or current directory if not specified).",
        "flags": {
          "help": "Show this help message.",
          "extension": "The file extension to look for (required)."
        }
      },
      "os": {
        "usage": "Usage: telocity os \"<prompt>\" [options]",
        "description": "Executes a single, one-shot prompt against an LLM. It can take context from a file (--file), standard input (stdin), and images (--image), then streams the LLM's output to the terminal.",
        "flags": {
          "help": "Show this help message.",
          "file": "Path to a text file to be appended to the prompt as context.",
          "image": "Path to image(s) for visual context. Supports glob patterns and comma-separated lists (e.g., \"img1.jpg,*.png\"). Supported formats: png, jpg/jpeg, gif, webp.",
          "params": "Select a model parameter preset (default: {{ .DefaultModel }}).",
          "model": "Override the model name specified in the preset.",
          "url": "Override the API endpoint URL.",
          "apikey": "Provide an API key for the request.",
          "reason": "Enable reasoning mode for presets that support it."
        },
        "footer": "Available Presets:\n{{ .ModelParamList }}"
      },
      "sp": {
        "usage": "Usage: telocity sp <source_path> <target_directory> [options]",
        "description": "Splits a large file into smaller parts based on a specified size, ensuring that lines are not broken.",
        "flags": {
          "help": "Show this help message.",
          "size": "The maximum size of each part in megabytes (default: {{ .Size }})."
        }
      },
      "st": {
        "usage": "Usage: telocity st <source_path> <target_path> [options]",
        "description": "Strips or extracts content between specified delimiters from a file.",
        "flags": {
          "help": "Show this help message.",
          "startdelimiter": "The starting delimiter.",
          "enddelimiter": "The ending delimiter.",
          "params": "Use start/end delimiters from a preset.",
          "extracttag": "Extract content between delimiters instead of removing it.",
          "compress": "Compress empty lines in the output."
        },
        "footer": "Available Presets with Tags:\n{{ .ReasoningTagParamList }}"
      },
      "tc": {
        "usage": "Usage: telocity tc <source_path> [options]",
        "description": "Counts the number of tokens in a file using a model-specific tokenizer.",
        "flags": {
          "help": "Show this help message.",
          "params": "The model preset whose tokenizer to use (default: {{ .DefaultModel }}).",
          "downloadmodel": "Download tokenizer files for a specific model (e.g., 'qwen')."
        },
        "footer": "Available Tokenizers:\n{{ .TokenParamList }}"
      },
      "tf": {
        "usage": "Usage: telocity tf <source_path> <target_path> [options]",
        "description": "Applies a transformation prompt to a source file, processing it in chunks. When using `--image`, the source file should contain the prompt for the image(s). In this mode, avoid setting a custom `--chunksize` to ensure the prompt is processed as a single unit.",
        "flags": {
          "help": "Show this help message.",
          "chunksize": "Number of lines per chunk (default: {{ .ChunkSize }}).",
          "batchsize": "Number of chunks to process per batch (default: {{ .BatchSize }}).",
          "parallel": "Concurrency limit (default: {{ .Parallel }}).",
          "prompt": "An optional instruction to prepend to the source file content. Generally omitted when using the --image flag.",
          "image": "Path to image(s) for visual context. Supports glob patterns and comma-separated lists (e.g., \"img1.jpg,*.png\"). Supported formats: png, jpg/jpeg, gif, webp.",
          "sysprompt": "Set a custom system prompt.",
          "params": "Select a model parameter preset (default: {{ .DefaultModel }}).",
          "model": "Override the model name from the preset.",
          "url": "Override the API endpoint URL.",
          "apikey": "Provide an API key.",
          "wait": "Set a delay between API calls.",
          "reason": "Enable reasoning mode for presets that support it."
        },
        "footer": "Available Presets:\n{{ .ModelParamList }}"
      },
      "tr": {
        "usage": "Usage: telocity tr <source_path> <target_path> [options]",
        "description": "Translates a file from a source language to a target language, processing it in chunks.",
        "flags": {
          "help": "Show this help message.",
          "chunksize": "Number of lines per chunk (default: {{ .ChunkSize }}).",
          "batchsize": "Number of chunks to process per batch (default: {{ .BatchSize }}).",
          "parallel": "Concurrency limit (default: {{ .Parallel }}).",
          "source": "Source language (default: \"{{ .SourceLanguage }}\").",
          "target": "Target language (default: \"{{ .TargetLanguage }}\").",
          "context": "Provide additional context for the translation.",
          "params": "Select a model parameter preset (default: {{ .DefaultModel }}).",
          "model": "Override the model name from the preset.",
          "url": "Override the API endpoint URL.",
          "apikey": "Provide an API key.",
          "wait": "Set a delay between API calls.",
          "reason": "Enable reasoning mode for presets that support it."
        },
        "footer": "Available Presets:\n{{ .ModelParamList }}"
      },
      "bg": {
        "usage": "Usage: telocity bg [options] <source_file> <target_jsonl_file>",
        "description": "Generates a JSONL file for batch processing translations from a source text file.",
        "flags": {
          "format": "The output format for the JSONL file. Supported: 'openai', 'gemini'. (default: openai)",
          "chunksize": "Number of lines per chunk/request. (default: {{ .ChunkSize }})",
          "model": "Overrides the model specified in the parameter set.",
          "params": "Parameter set to use for prompt and model configuration. (default: {{ .DefaultModel }})",
          "source": "Source language for translation prompts.",
          "target": "Target language for translation prompts.",
          "context": "Additional context to include in the prompt for each chunk.",
          "reason": "Use the 'reasoning' variant of the selected model parameter set.",
          "help": "Display this help message."
        },
        "footer": "\nAvailable Parameter Sets:\n{{ .ModelParamList }}"
      },
      "ch": {
        "usage": "Usage: llm-cli ch [options] <chat-name>",
        "description": "Starts an interactive chat session with an LLM. Chat history and configuration are saved to a file named <chat-name>.json in the application's state directory.",
        "flags": {
          "file": "Attaches a text file to the first message in a new chat session.",
          "image": "Attaches one or more image files (comma-separated or glob pattern) to the first message in a new chat session.",
          "model": "Override the model specified by the parameter set.",
          "params": "The parameter preset to use for the LLM. See list below.",
          "context-limit": "Set the maximum token limit for the chat context.",
          "apikey": "API key for the LLM service.",
          "url": "Override the URL for the LLM service.",
          "help": "Display this help message."
        },
        "footer": "Inside the chat, type /help to see available interactive commands like /exit, /stats, /delete, /browse, and /insert.\n\nAvailable Parameter Sets:\n{{ .ModelParamList }}"
      }
    }
  }
}
